## Hi There ðŸ‘‹
I'm Kaicheng Yang, an algorithm researcher at DeepGlint. In 2021, I earned my masterâ€™s degree from Hebei University of Science and Technology. During 2018 to 2020, I had the opportunity to conduct research as a visiting student at Tsinghua University under the mentorship of Professor Hua Xu. My research interests include multimodal representation learning, vision-language pretraining, and contrastive learning.

Iâ€™m actively hiring interns in the fields of multimodal representation learning, large-scale data cleaning, RAG, and Agent. If you want to collaborate with me, please feel free to contact meðŸ˜Š.

Email:kaichengyang0828@gmail.com; kaichengyang@deepglint.com.

## Recent NewsðŸ”¥
- *2025.10*: We present the [UniME-V2](https://arxiv.org/pdf/2510.13515).
- *2025.09*: We present the [LLaVA-OneVision-1.5](https://arxiv.org/abs/2509.23661).
- *2025.09*: ðŸŽ‰ UniViT have been accepted by NeurIPS2025.
- *2025.08*: ðŸŽ‰ [GA-DMS](https://arxiv.org/pdf/2509.09118) has been accepted by EMNLP2025 Main.
- *2025.07*: ðŸŽ‰ Three paper(RealSyn, UniME, DeGLA) have been accepted by ACM MM2025.
- *2025.06*: ðŸŽ‰ Three paper(RICE, ForCenNet, HUST) have been accepted by ICCV2025.
- *2025.06*: ðŸŽ‰ DLOV-3D has been accepted by IROS2025.
- *2025.04*: We present the [DeGLA](https://arxiv.org/abs/2504.16801) and [UniME](https://arxiv.org/pdf/2504.17432).
- *2025.02*: We present the [RealSyn](https://arxiv.org/abs/2502.12513).
- *2024.12*: ðŸŽ‰[CLIP-CID](https://arxiv.org/pdf/2408.09441) has been accepted by AAAI2025.
- *2024.11*: ðŸŽ‰ [ORID](https://arxiv.org/pdf/2411.13025) has been accepted by WACV2025.
- *2024.10*: We present the [Croc](https://arxiv.org/abs/2410.14332).
- *2024.09*: ðŸŽ‰ [RWKV-CLIP](https://arxiv.org/abs/2406.06973) has been accepted by EMNLP2024(Main).
- *2024.07*: ðŸŽ‰ [MLCD](https://arxiv.org/pdf/2407.17331) has been accepted by ECCV2024.
- *2024.06*: We present the [High-Fidelity Facial Albedo](https://arxiv.org/pdf/2406.13149).
- *2024.04*: Our team achieve 1st Place on the [1st SkatingVerse Challenge(18th IEEE International Conference on Automatic Face and Gesture Recognition workshop)](https://skatingverse.github.io/).
- *2024.03*: ðŸŽ‰ [LaPA](https://arxiv.org/pdf/2404.13039.pdf) has been accepted by CVPRW2024.
- *2023.09*: I will attend ICCV2023, happy to see you in ICCV (Paris) in person.
- *2023.09*: ðŸŽ‰ Our team achieve co-winner in [OpenFAD Challenge(ICCV2023 workshop)](https://openfad.nist.gov).
- *2023.07*: ðŸŽ‰ [ALIP](https://arxiv.org/abs/2308.08428) has been accepted by ICCV2023.
- *2023.05*: I will attend ICLR2023, happy to see you in ICLR (Kigali) in person.
- *2023.01*: ðŸŽ‰ [Unicom](https://arxiv.org/pdf/2304.05884.pdf) has been accepted by ICLR2023.
- *2022.10*: We achieve 11/1022(top 1%) in the [Google Universal Image Embedding Challenge(ECCV2022 Instance-Level Recognition workshop)](https://www.kaggle.com/competitions/google-universal-image-embedding/overview/eccv-2022).
- *2020.10*: ðŸŽ‰ CM-BERT has been accepted by ACM MM2020(**Oral**).
- *2020.07*: ðŸŽ‰ CH-SIMS has been accepted by ACL2020.

## Thank you for the visit.
![](http://profile-counter.glitch.me/Kaicheng-Yang0828/count.svg)

Please give me a starðŸŒŸ if you like.
<!--
**Kaicheng-Yang0828/Kaicheng-Yang0828** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
